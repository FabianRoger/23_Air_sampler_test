---
title: "Insect COI"
author: "Fabian Roger"
date: "`r Sys.Date()`"
output: html_notebook
---

```{r, message = FALSE}

#data handling
library(dplyr)
library(tidyr)
library(readr)
library(here)
library(readxl)
library(stringr)
library(googlesheets4)

#plotting
library(ggplot2)
library(gridExtra)
library(ggrepel)


#seqeunce analysis
library(dada2)
library(ShortRead)
library(DECIPHER)
library(taxize)
library(rgbif)
library(phyloseq)

#library(knitr)

#parallel processing
library(future.apply) 
library(furrr)
#library(rgbif)
#library(ggtree)
#library(ape)


#library(decontam)
#library(googlesheets4)
#library(lulu)
#library(bold)

#helper functions for sequence analyis
source(here("Scripts", "Seq_analysis_helper.R"))
```

```{r}
sessionInfo()
```


#dada2
This script follows the [DADA2 ITS Tutorial (1.8)](https://benjjneb.github.io/dada2/ITS_workflow.html) 
The original article can be found [here](http://rdcu.be/ipGh)

The script was written with DADA2 version 1.18.0

The data are demultiplexed (see `Demultiplexing.Rmd`)

#set-up

here("Data", "iSeq", "20231128_FS10000965_18_BSB09423-0723", "phenix_config_file.json")

```{r}
run_name <- "20231128_FS10000965_18_BSB09423-0723"
primer <- "Insects"

#path to primer demultiplexed files
trimmed_path <- here("Data", "iSeq", run_name, primer)
```

#Filter

filter trimmed reads
```{r}
filterpath <- here("Data", "iSeq", run_name, primer, "filtered")
dir.create(filterpath)
```


```{r, "sort fwrd and rev", cache=TRUE}

fnFs <- sort(list.files(trimmed_path, pattern="_R1.fastq.gz", full.names = TRUE)) # Just the forward read files
fnRs <- sort(list.files(trimmed_path, pattern="_R2.fastq.gz", full.names = TRUE)) # Just the reverse read files

# Get sample names from forward read filenames
sample.names <- 
   sapply(fnFs, function(x) gsub(".+Insects/(.+)_R1.fastq.gz", "\\1", x), USE.NAMES = FALSE)

```

###quality profiles

Visualize the quality profile of the forward reads

```{r, "plot quality of reads", cache=TRUE}
# plot one forward and one reverse read in the rapport
plotQualityProfile(fnFs[152])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[1], "foward", sep = " - " ))

plotQualityProfile(fnRs[152])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[1], "reverse", sep = " - "))
```

The quality of the forwards reads is very high, throughout the full length of the read.  The quality of the reverse read is equally high


### merged read length
To decide on appropriate trimming parameters, we check the distribution of the merged read-lengths.

we don't need the merged reads (DADA2 needs unmerged reads) so we merge them on them fly and just store the length distributions


```{r}
#merge first file with vsearch

  system(
  paste(
    "/Applications/vsearch/bin/vsearch --fastq_mergepairs ", fnFs[152], 
    " --reverse ", fnRs[152], 
    " --fastaout", here("Data", "iSeq", run_name, primer, "test_merge.fasta"), 
    "--fastq_allowmergestagger"
    ), intern = TRUE
  )
  
```

We sequenced with 2 x 150 bp but the fragment size is > 300 bps. We cannot merge the sequences so I only analyse the forward reads

The filtering parameters we’ll use are standard (however we can allow for more expected errors as DADA is [robust to low quality sequences](https://twitter.com/bejcal/status/771010634074820608) )

+ maxN=0 (DADA2 requires no Ns)
+ truncQ=2 
+ maxEE=c(2,2)

The `maxEE` parameter sets the maximum number of “expected errors” allowed in a read. 

We use the `fastqPairedFilter` function to jointly filter the forward and reverse reads.

###filter and trim
```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(filterpath, paste0(sample.names, "_F_filt.fastq.gz"))
```


```{r}
out <- filterAndTrim(fnFs, filtFs,
                     maxN=0,
                     maxEE=2,
                     truncQ=2,
                     rm.phix=TRUE,
                     compress=TRUE, multithread=6) 

out_df <- 
out %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "Sample_name") %>% 
  mutate(Sample_name = gsub("_R1.+", "", Sample_name)) %>% 
  mutate(prct = signif(reads.out / reads.in * 100, 2)) %>% 
  mutate(Sample_name = gsub("(.+?)_S.+.fastq.gz", "\\1", Sample_name))

out_df
```


```{r}
filt <- list.files(filterpath, full.names = T)

filtFs <- filt[grepl("*_F_filt.fastq.gz", filt)]
names(filtFs) <- gsub(".+filtered/(.+?)_F.+", "\\1", filtFs)
```


# Learn errors
```{r}
# Learn errors
errF <- learnErrors(
  filtFs,
  multithread = TRUE,
  nbases = 1e10,
  errorEstimationFunction = loessErrfun_mod4,
  verbose = TRUE
)
```


```{r, cache=TRUE}
plotErrors(errF, nominalQ=TRUE)
```

# Denoise
```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
```

```{r}

denoise_stat <- 
sapply(dadaFs, function(x) sum(getUniques((x)))) %>% 
  data.frame(denoised = .) %>% 
  tibble::rownames_to_column(var = "Sample_name") %>% 
  left_join(select(out_df, Sample_name, reads.out)) %>% 
  mutate(prct = round(denoised / reads.out*100, 2))


denoise_stat
```



# Construct sequence table:

```{r, "Seqeunce table"}
seqtab <- makeSequenceTable(dadaFs)
dim(seqtab)
```


# Chimeras

```{r, "Chimera removal", cache=TRUE}

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)


dim(seqtab.nochim)

```

```{r}
chimera_stat <- 
tibble(Sample_name = rownames(seqtab),
       before = rowSums(seqtab),
       after = rowSums(seqtab.nochim),
       prct = round(after/before*100,2))

chimera_stat %>% 
write_tsv(here("Data", "iSeq", run_name, primer, "Chimera_stat.txt"))

chimera_stat
```


percentage of Chimeric reads

```{r}
(1-(sum(seqtab.nochim)/sum(seqtab)))*100
```

percentage of Chimeric ASVs

```{r}
(1-(ncol(seqtab.nochim)/ncol(seqtab)))*100
```


# Length filtering

check ASV length

```{r}
data.frame(table(nchar(colnames(seqtab.nochim)))) %>% 
  ggplot( aes(x = Var1, y = Freq))+
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = -90))+
  labs(x = "length in bp", title = "histogramm of merged sequence lenghts")
```

###export raw ASV and fasta file

export ASVs to Fasta file 

```{r}
seqs <- DNAStringSet(colnames(seqtab.nochim))

names(seqs) <- paste("seq", 
                             formatC(1:length(seqs), flag = "0",
                                     width = nchar(length(seqs))),
                             sep = "_")

writeXStringSet(seqs, here("Data", "iSeq", run_name, primer, "ASVs.fasta"), format = "fasta")

#seqs <- readDNAStringSet(here("Data", run_name, primer, "ASVs.fasta"))

ASV <- 
seqtab.nochim %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "Sample") %>% 
  `colnames<-`(c("Sample", names(seqs))) 
  
write_tsv(ASV, here("Data", "iSeq", run_name, primer, "ASVs.txt"))
#ASV <- read_tsv(here("Data", run_name, primer, "ASVs.txt"))
```


###Swarm3

We use swarm for single linkage clustering of the ASVs. The method is described here:

> Mahé F, Rognes T, Quince C, Vargas C de, Dunthorn M. 2015 Swarm v2: highly-scalable and high-resolution amplicon clustering. PeerJ 3, e1420. (doi:10.7717/peerj.1420)

The github is [here](https://github.com/torognes/swarm)


Swarm needs a fasta file with size annotation as input. It just separates the size by a "_"

```{r}
seqs_size <- 
  colSums(ASV[,-1]) %>% 
  data.frame(size = .) %>% 
  tibble::rownames_to_column(var = "seq") %>% 
  mutate(new_name = gsub("_", "", seq)) %>% 
  mutate(new_name = paste(new_name, size, sep = "_"))

#make sure sequences are in correct order
seqs <- seqs[seqs_size$seq]

#rename
names(seqs) <- seqs_size$new_name

#export
writeXStringSet(seqs, here("Data", "iSeq", run_name, primer, "ASVs_swarm_in.fasta"), format = "fasta")

```


run swarm.

we use the usearch output as we already have code to convert usearch output into merged OTU tabel

```{r}
tmp <- 
system(paste(
  "/Applications/swarm-3.1.0/bin/swarm -d 1 -f -t 8 -w",
  here("Data", "iSeq", run_name, primer, "ASVs_swarm_out.fasta"), 
  "-u", here("Data","iSeq", run_name, primer, "ASVs_swarm_cluster.txt"),
  here("Data", "iSeq", run_name, primer, "ASVs_swarm_in.fasta")
  , sep = " ")
  , intern = TRUE)

```


The ITS_ASVs_swarm_cluster.txt file tells us which sequences have been clustered

column 09 gives the original sequence name
column 10 gives the centroid sequence to which it has been clustered
column 01 tells us if the sequence is itself a centroid sequence ("S") or clustered to a centroid ("H")
column 04 gives the % identity with the centroid (here between 100% and 99%)

```{r}

Cluster_ID <- read_delim(here("Data", "iSeq", run_name, primer, "ASVs_swarm_cluster.txt"), delim = "\t", col_names = FALSE)

#filter only sequences that have been clustered
Cluster_ID <- 
Cluster_ID %>% 
  dplyr::rename(seq = X9, Type = X1, match = X10, pident = X4) %>% 
  select(seq, Type, match, pident) %>% 
  filter(Type == "H" ) %>% 
  arrange(match)

#rename seqs to match sequence table
Cluster_ID <- 
Cluster_ID %>% 
  mutate(across(one_of("seq", "match"), function(x){
    gsub("(seq)(\\d+)_\\d+", "\\1_\\2", x)}))

#split by cluster centroid
Cluster_list <- split(Cluster_ID, Cluster_ID$match)


#sum abundance of clustered sequences
ASV_list <- 
  lapply(Cluster_list, function(x) {
  S_seq <- unique(x$match)
  C_seqs <- x$seq
  ASV[,S_seq] <- rowSums(ASV[,c(S_seq,C_seqs)])
})

#create new ASV table with only the centroid sequences
ASV_sw <- ASV
ASV_sw[,names(ASV_list)] <- bind_rows(ASV_list)
ASV_sw <- ASV_sw[, which(! colnames(ASV_sw) %in% Cluster_ID$seq)] 

#export ASV table
ASV_sw %>% 
  write.table(., here("Data","iSeq",  run_name, primer, "ASV_swarm.text"))

```

update fasta file with sequence centroids
```{r}
seqs <- readDNAStringSet(here("Data", "iSeq", run_name, primer, "ASVs_swarm_out.fasta"))

names(seqs) <- gsub("(seq)(\\d+)_\\d+", "\\1_\\2", names(seqs))

sum(!names(seqs) %in% colnames(ASV_sw[,-1]))

writeXStringSet(seqs, here("Data","iSeq", run_name, primer, "ASV_swarm.fasta"))
```

read_files
```{r}
seqs <- readDNAStringSet(here("Data","iSeq", run_name, primer,  "ASV_swarm.fasta"))

ASV_sw <- read.table(here("Data", "iSeq", run_name, primer, "ASV_swarm.text"))
```

#taxonomy

1.Leray M, Knowlton N, Machida RJ. In press. MIDORI2: A collection of quality controlled, preformatted, and regularly updated reference databases for taxonomic assignment of eukaryotic mitochondrial sequences. Environmental DNA n/a. (doi:10.1002/edn3.303)


##Sintax

```{r}
system(paste(
  
  "/Applications/vsearch/bin/vsearch --sintax", here("Data", "iSeq", run_name, primer, "ASV_swarm.fasta"), "--db", here("Data", "MIDORI2_LONGEST_NUC_GB252_CO1_SINTAX.fasta"), "--sintax_cutoff 0.1 --tabbedout", here("Data","iSeq", run_name, primer, "COI_midori_tax.txt"), sep = " "

))
```

```{r}
Sintax_taxa <- read_tsv(here("Data","iSeq", run_name, primer, "COI_midori_tax.txt"), 
                        col_names = FALSE) %>% 
  select(1:2)

colnames(Sintax_taxa) <- c("seq", "tax")

Sintax_prep <- 
  Sintax_taxa %>% 
  mutate(tax = strsplit(tax, ",")) %>% 
  unnest(tax) %>% 
  group_by(seq) %>% 
  dplyr::slice(-1) %>% 
  mutate(taxonomy_name = gsub("\\w:(.+)_.+", "\\1", tax),
         taxonomy_level = gsub("(\\w):.+", "\\1", tax),
         taxonomy_conf = gsub(".+\\((.+)\\)", "\\1", tax)) %>% 
  mutate(taxonomy_level = case_when(taxonomy_level == "p" ~ "phylum",
                   taxonomy_level == "c" ~ "class",
                   taxonomy_level == "o" ~ "order",
                   taxonomy_level == "f" ~ "family",
                   taxonomy_level == "g" ~ "genus",
                   taxonomy_level == "s" ~ "species"))

#get table with all assignments (even the unlikely ones)
Sintax_taxa_table_full <- 
 Sintax_prep %>% 
  select(-tax, -taxonomy_conf) %>% 
  pivot_wider(names_from = taxonomy_level, values_from = taxonomy_name)

#get table with probabilities for each assignment
Sintax_taxa_table_prob <- 
  Sintax_prep %>% 
  select(-tax, -taxonomy_name) %>% 
  pivot_wider(names_from = taxonomy_level, values_from = taxonomy_conf)
```

export

```{r}
write_tsv(Sintax_taxa_table_full, here("Data", "iSeq", run_name, primer,  "COI_Sintax_tax.txt"))
write_tsv(Sintax_taxa_table_prob, here("Data", "iSeq", run_name, primer,  "COI_Sintax_prob.txt"))

Sintax_taxa_table_full <- read_tsv(here("Data", "iSeq", run_name, primer,  "COI_Sintax_tax.txt"))
Sintax_taxa_table_prob <- read_tsv(here("Data", "iSeq", run_name, primer,  "COI_Sintax_prob.txt"))
```


```{r}
COI_taxa_80 <- Sintax_taxa_table_full
COI_taxa_80[Sintax_taxa_table_prob < 0.8] <- NA_character_ 

COI_taxa_80 <- 
  COI_taxa_80 %>% 
  mutate(across(everything(), function(x){gsub("NA", NA, x, fixed = TRUE)}))

write_tsv(COI_taxa_80, here("Data", "iSeq", run_name, primer, "COI_Sintax_80.txt"))
COI_taxa_80 <- read_tsv(here("Data", "iSeq", run_name, primer, "COI_Sintax_80.txt"))
```

```{r}
COI_taxa_80 %>% 
  group_by(phylum) %>% 
  summarise(n = n())

COI_taxa_80 %>% 
  filter(phylum == "Arthropoda") %>% 
  filter(!is.na(order))
```

## merge species

cluster sequences assigned to same species or genus (if species is NA)

collapse reads from same species
```{r}
#show same genus / species assigned to more than one OTU
  COI_taxa_80 %>% 
  group_by(genus, species) %>% 
  filter(n() > 1) %>% 
  arrange(genus, species)
```

we use phyloseq to merge ASVs with teh same taxonomic assignment at species level, or genius level if a species level assignment is missing

```{r}

ASV_ps <- ASV_sw[,-1]
rownames(ASV_ps) <- ASV_sw$Sample

COI_taxa_80_ps <- 
  COI_taxa_80 %>% 
   mutate(species = case_when(is.na(species) & !is.na(genus) ~ paste(genus, "sp."),
                             TRUE ~ species)) %>% 
  select(-seq) %>% 
  as.matrix()

rownames(COI_taxa_80_ps) <- COI_taxa_80$seq

ps_obj <- phyloseq(otu_table(ASV_ps, taxa_are_rows = FALSE),
                   tax_table(COI_taxa_80_ps))

ps_glom <- tax_glom(ps_obj, taxrank="species", NArm=FALSE)

COI_taxa_80_glom <- 
  as.data.frame(tax_table(ps_glom)) %>% 
  tibble::rownames_to_column("seq")

ASW_glom <- 
  as.data.frame(otu_table(ps_glom)) %>% 
  tibble::rownames_to_column("Sample")

```


export
```{r}
write_tsv(COI_taxa_80_glom, here("Data", "COI_taxa_80_glom.txt"))

write_tsv(ASW_glom, here("Data", "COI_ASW_glom.txt"))
```

```{r}
COI_taxa_80_glom <- read_tsv(here("Data", "COI_taxa_80_glom.txt"))

ASW_glom <- read_tsv(here("Data", "COI_ASW_glom.txt"))
```


#Analysis
```{r}
gURL <- "https://docs.google.com/spreadsheets/d/1GKWzD0q683oH3I_i5ueulbf4P4Q-9-a9YPbdyUZyF2A/edit#gid=847966271"

Meta <- read_sheet(gURL, sheet = "Sample_data")

Plates <- read_sheet(gURL, sheet = "Plates", range = "A1:M21", col_types = "c") %>% 
  dplyr::rename(row = 1) %>% 
  filter(row %in% LETTERS[1:8]) %>% 
  mutate(Plate = rep(c("Plate_1", "Plate_2"), each = 8)) %>% 
  pivot_longer(!one_of(c("row", "Plate")), names_to = "col", values_to = "Sample_names") %>% 
  select(Plate, row, col, Sample_names)

shini_barcodes <- 
  read_xlsx(here("Documents", "Shini_barcodes_4_plates.xlsx")) %>% 
  mutate(Plate = rep(rep(paste("Plate", 1:4, sep = "_"), each = 96),2)) %>% 
  filter(Plate %in% c("Plate_1", "Plate_2")) %>% 
  dplyr::rename(Index = 1) %>% 
  select(Index, Plate) %>% 
  filter(grepl("_i5$", Index)) %>% 
  mutate(Index = gsub("(.+?)_i5", "\\1", Index)) %>% 
  group_by(Plate) %>% 
  mutate(row = rep(LETTERS[1:8], each = 12)) %>% 
  mutate(col = as.character(rep(1:12, 8)))
```


```{r}
Meta <- 
Plates %>% 
  filter(!is.na(Sample_names)) %>% 
  left_join(shini_barcodes) %>% 
  select(Sample_names, Index) %>% 
  left_join(Meta)

ASV_tax_long <- 
ASW_glom %>% 
  pivot_longer(-Sample, names_to = "seq", values_to = "reads") %>% 
  dplyr::rename(Index = Sample) %>% 
  left_join(Meta) %>% 
  left_join(COI_taxa_80_glom)
```

```{r}

ASV_tax_long %>% 
  mutate(reads = ifelse(reads == 0, NA, reads)) %>% 
  arrange(Type, Sites, duration) %>% 
 # filter(Sampler == "MWAC") %>% 
  mutate(Sample_names = factor(Sample_names, levels = unique(.$Sample_names))) %>% 
  ggplot(aes(y = species, x = Sample_names, size = reads, colour = Type))+
  geom_point()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
  
```


```{r}
phylum_reads <- 
ASW_glom %>% 
  pivot_longer(-Sample, names_to = "seq") %>% 
  group_by(seq) %>% 
  summarise(reads = sum(value)) %>% 
  left_join(COI_taxa_80_glom) %>% 
  group_by(phylum) %>% 
  summarise(reads = sum(reads)) %>% 
  mutate(prct_reads = round(reads / sum(reads)*100,2)) %>% 
  mutate(phylum = case_when(prct_reads < 0.5 ~ "other",
                            TRUE~phylum)) %>% 
  group_by(phylum) %>% 
  summarise(reads = sum(reads)) %>% 
  mutate(prct_reads = round(reads / sum(reads)*100,2)) %>% 
  arrange(desc(prct_reads))

phylum_asv <- 
  COI_taxa_80_glom %>% 
  mutate(phylum = case_when(phylum %in% phylum_reads$phylum ~ phylum,
                            TRUE~"other")) %>% 
  group_by(phylum) %>% 
  summarise(n = n()) %>%
  mutate(prct_asv = round(n / sum(n)*100,2)) 

phylum_factor <- 
  phylum_reads %>% 
  arrange(desc(prct_reads)) %>% 
  pull(phylum)

phylum_reads %>% 
  left_join(phylum_asv) %>% 
  select(phylum, prct_reads, prct_asv) %>% 
  pivot_longer(-phylum) %>% 
  mutate(phylum = factor(phylum, levels = phylum_factor)) %>% 
  ggplot(aes(x = phylum, y = value, fill = name))+
  geom_bar(stat = "identity", position = position_dodge())+
  scale_fill_brewer(palette = "Set1")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
  
```

```{r}
ASW_glom %>% 
  pivot_longer(-Sample, names_to = "seq") %>% 
  mutate(value = ifelse(value > 0, 1, 0)) %>% 
  group_by(seq) %>% 
  summarise(value = sum(value)) %>% 
  arrange(desc(value)) %>% 
  left_join(COI_taxa_80_glom)

seqs <-  readDNAStringSet(here("Data", "iSeq", run_name, primer, "ASVs.fasta"))

seqs[c(1,11)] %>% as.character()
```



