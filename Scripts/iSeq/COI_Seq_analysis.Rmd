---
title: "COI"
author: "Fabian Roger"
date: "`r Sys.Date()`"
output: html_notebook
---

```{r, message = FALSE}

#data handling
library(dplyr)
library(tidyr)
library(readr)
library(here)
library(readxl)
library(stringr)

#plotting
library(ggplot2)
library(gridExtra)
library(ggrepel)


#seqeunce analysis
library(dada2)
library(ShortRead)
library(DECIPHER)

#library(knitr)

#parallel processing
library(future.apply) 

#library(furrr)
#library(rgbif)
#library(ggtree)
#library(ape)


#library(decontam)
#library(googlesheets4)
#library(lulu)
#library(bold)

#helper functions for sequence analyis
source(here("Scripts", "Seq_analysis_helper.R"))
```

```{r}
sessionInfo()
```


#dada2
This script follows the [DADA2 ITS Tutorial (1.8)](https://benjjneb.github.io/dada2/ITS_workflow.html) 
The original article can be found [here](http://rdcu.be/ipGh)

The script was written with DADA2 version 1.18.0

The data are demultiplexed (see `Demultiplexing.Rmd`)

#set-up

```{r}
run_name <- "230207_M06272_0125_000000000-KPLCR_p916"
primer <- "COI"

Sample_meta <- read_xlsx(here("Data", "Sample_sheet_Masoala_Air-Surface_January2023.xlsx"),
                         sheet = 1)

#path to demultiplexed sequences
path_demulti <- here("Data", run_name, "demultiplexed")

#create directories for COI analysis
path_primer <- here("Data", run_name, primer)
dir.create(path_primer)

#path to plots
plotpath <- here("Data", run_name, primer, "plots")
dir.create(plotpath)
```



# primer seq

<!-- ITS2F - ATGCGATACTTGGTGTGAAT  -->
<!-- ITS3R - GACGCTTCTCCAGACTACAAT -->

<!-- For Vertebrates:  -->
<!-- P5_Ve16S_F <- "ACACTCTTTCCCTACACGACGCTCTTCCGATCTCGAGAAGACCCTATGGAGCTTA" -->
<!-- P7_Ve16S_R <- "GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCTAATCGTTGAACAAACGAACC" -->

<!-- Birds specifically: -->
<!-- Bird Fwrd : YGGTAAATCYTGTGCCAGC -->
<!-- Bird Rev : AAGTCCTTAGAGTTTYAAGCGTT -->

Insects: 
Sauron-S878 : GGDRCWGGWTGAACWGTWTAYCCNCC
jgHCO2198 : TAIACYTCIGGRTGICCRAARAAYCA


```{r}
Fwrd_primer <- "GGDRCWGGWTGAACWGTWTAYCCNCC"

Rev_primer <- "TAIACYTCIGGRTGICCRAARAAYCA"

#substitute I with N as it Deoxyinosine (I) functions as universal base in this primer
Rev_primer <- gsub("I", "N", Rev_primer)
```

```{r}
nchar(Fwrd_primer)
nchar(Rev_primer)
```


```{r}
Sample_meta_min <- 
Sample_meta %>% 
  dplyr::rename(Sample_name = "Sample-ID",
         Sample_type = "sample-type") %>% 
  select(Sample_name, Sample_type) %>% 
  filter(!is.na(Sample_name))

Sample_meta_min
```


# Trimming

Trimming will do three things 

1) it will recognize the primer sequences at the start of the reads and trim them

2) (in this case) it will recognize the reverse complement of the primers at the end of the read 

3) it will discard sequences that are too short (e.g. primer dimers). As quality control and to get a feeling for the samples, we check the length distribution of the reads in the different files.

We check where the primers are in the reads and in what orientation

The presence of ambiguous bases (Ns) in the sequencing reads makes accurate mapping of short primer sequences difficult. We are going to “pre-filter” the sequences just to remove those with Ns, but perform no other filtering.

```{r}

#list files
dm_files <- list.files(path_demulti, full.names = F)

#sort in forward and reverse reads
forward_raw <- dm_files[grepl("_R1.fastq.gz", dm_files)]
reverse_raw <- dm_files[grepl("_R2.fastq.gz", dm_files)]

#create directories for N filtered sequences
N_path <- here("Data", run_name, primer, "N_filtred")
dir.create(N_path)

# Get sample names from forward read filenames
sample.names <- 
  sapply(forward_raw, function(x) gsub("(.+)_R1.fastq.gz", "\\1", x), USE.NAMES = FALSE)
```


## filter N
pre-filter sequences to remove sequences with Ns

```{r}
# Place filtered files in N_filtered subdirectory
filtFs <- file.path(N_path, paste0(sample.names, "_R1_Nfilt.fastq.gz"))
filtRs <- file.path(N_path, paste0(sample.names, "_R2_Nfilt.fastq.gz"))

# path to input files
fnFs <- here(path_demulti, forward_raw)
fnRs <- here(path_demulti, reverse_raw)

names(filtFs) <- sample.names
names(filtRs) <- sample.names
```


```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     maxN=0,
                     rm.phix=c(TRUE,TRUE),
                     compress=TRUE, 
                     multithread=6)
```

samples with no reads left after N filtering

```{r}

N_filter_stats <- 
out %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "Sample_name") %>% 
  mutate(Sample_name =  gsub("(.+)?_S.+.fastq.gz", "\\1", Sample_name)) %>% 
  mutate(prct_N = round(100 - reads.out/reads.in*100,2)) %>% 
  left_join(Sample_meta_min) %>% 
  arrange(Sample_type)

N_filter_stats %>% 
write_tsv(here("Data", run_name, primer, "N_filter_stats.txt"))

N_filter_stats
```


## look for primers

get all orientations of the primers
```{r}
Fwrd.orients <- allOrients(Fwrd_primer) 
Rev.orients <- allOrients(Rev_primer)
```

check for primers one true sample file, avoiding negative controls. 

```{r}
test_sample <- 
N_filter_stats  %>% 
  arrange(reads.out) %>% 
  dplyr::slice(floor(n()/2)) %>% 
  mutate(Sample_name = gsub(".fastq.gz", "", Sample_name)) %>% 
  pull(Sample_name)

test_sampleF <- filtFs[grepl(test_sample, filtFs)]
test_sampleR <- filtRs[grepl(test_sample, filtFs)]
```


```{r}
Primer_on_reads <- 
rbind(FWD.ForwardReads = sapply(Fwrd.orients, primerHits, 
                                fn = test_sampleF), 
    FWD.ReverseReads = sapply(Fwrd.orients, primerHits, 
                              fn =test_sampleR), 
    REV.ForwardReads = sapply(Rev.orients, primerHits, 
                              fn = test_sampleF), 
    REV.ReverseReads = sapply(Rev.orients, primerHits, 
                              fn = test_sampleR))

Primer_on_reads
```

We find the forward primer on the first read and the reverse primer on the second read - both in forward orientation. There is no read-through.  

--> we need to trim primers only from the start of the read

## trim primer
I use [cutadapt](https://cutadapt.readthedocs.io/en/stable/guide.html) to trim the primers from the sequences and filter only sequences with detectable Primers. 

setting up variables:

I read in the N filtered sequencing files  and sort them into first and second reads

I also create an output folder for the filtered sequences
```{r}

#list files (filtered reads without N's)
N_files <- list.files(N_path, full.names = F)

#sort in forward and reverse reads
forward_noN <- N_files[grepl("_R1_Nfilt.fastq.gz", N_files)]
reverse_noN <- N_files[grepl("_R2_Nfilt.fastq.gz", N_files)]

#create directories for filtered sequences
trimmed_path <- here("Data",run_name, primer, "trimmed")

dir.create(trimmed_path)
```


before we set the minimum length, we check the length distribution of the reads. In theory all should have the roughly the same length (300bp) and much shorter reads are unlikely to be true sequences but rather primer dimers

### check length
```{r}

fwrd_length <- system(paste("zcat <", test_sampleF ," | awk '{if(NR%4==2) print length($1)}' | sort -n | uniq -c", sep = ""), intern = T)

length_df <- 
fwrd_length %>%  
  as_tibble()

length_df <- 
length_df %>% 
  mutate(value = gsub("^\\s+", "", value)) %>% 
  tidyr::separate(value, sep = " ", into = c("freq", "length")) %>% 
  mutate(across(one_of(c("freq", "length")), as.numeric))
```


```{r}
length_df %>% 
  ggplot(aes(y = freq, x = length))+
  geom_point(size = 1)+
  geom_line(aes(group = 1), size = 0.5, colour = "grey")+
  theme_bw()
```

Almost all reads seem to have the expected 300 bp read length. The Primer length is 26 bp so the expected length after trimming is 274 bp. We choose 250 bp as minimum. 

### trim
```{r}
#minimum length for sequences after trimming (shorter sequences are discarded)

minlength <- 250
```


run cutadapt 

-g specifies that the primer adaptor is at the 5' end of the read.

> 5’ adapters preceed the sequence of interest

```{r}

#number of cores
Cores <- 6

cutadapt_out <- vector(mode = "list", length = length(forward_noN))
names(cutadapt_out) <- names(fwrd_length)

for (i in seq_along(forward_noN)) {
  
  message(paste("processing file", i, "out of", length(forward_noN), sep=" "))

# will sort COI in separate file below
temp <- 
system(
paste('/usr/local/cutadapt/bin/cutadapt -a ', #replace by your path to cutadapt
      '"', Fwrd.orients['Forward'],';required;min_overlap=12','"','...', Rev.orients['RevComp'],
      ' -A ', '"',Rev.orients['Forward'],';required;min_overlap=12','"', '...', Fwrd.orients['RevComp'],
      ' --discard-untrimmed',
      ' --minimum-length=', minlength, #minimum length for sequences after trimming
      ' --cores=', Cores, # Number of cores to use
      ' -o ', paste(trimmed_path, '/', forward_noN[i], sep = ''),
      ' -p ', paste(trimmed_path, '/', reverse_noN[i], sep = ''),
      paste(' ', N_path, '/', forward_noN[i], sep = ''),
      paste(' ', N_path, '/', reverse_noN[i], sep = ''),
      sep = ''
      ), intern = TRUE
)

cutadapt_out[[i]] <- Cutadapt_parser(temp)
}

cutadapt_out_df <- bind_rows(cutadapt_out, .id = "Sample_name")

cutadapt_out_df %>% 
  mutate(Sample_name = sample.names) %>% 
  write_tsv(here("Data", run_name, primer, "Primer_trimming_stats.txt"))

cutadapt_out_df
```



around 50% of the reads are written for most samples, which is expected as the libraries were pooled such that only about 50% of the reads should come from COI with the other 50% being from the two other markers 


### check length
```{r}

fwrd_trimmed_length <- 
system(paste("zcat <", paste(trimmed_path, "/", 
                             forward_noN[grepl(test_sample, forward_noN)], 
                             sep = ""),
             " | awk '{if(NR%4==2) print length($1)}' | sort -n | uniq -c", sep = ""),
       intern = T)


length_trimmed_df <- 
  fwrd_trimmed_length %>% 
  as_tibble() %>%
  mutate(value = gsub("^ +", "", value)) %>% 
  tidyr::separate(value, sep = "\\s", into = c("freq", "length")) %>% 
  mutate(across(one_of(c("freq", "length")), as.numeric))
```


```{r}
length_trimmed_df %>% 
  ggplot(aes(y = freq, x = length))+
  geom_line(aes(group = 1), size = 0.5, colour = "grey")+
  theme_bw()
```

the trimming worked as expected and the trimmed length is the expected length of 274 / 275 bp


#Filter

filter trimmed reads
```{r}
filterpath <- here("Data", run_name, primer, "filtered")
dir.create(filterpath)
```


```{r, "sort fwrd and rev", cache=TRUE}

fnFs <- sort(list.files(trimmed_path, pattern="_R1_Nfilt.fastq.gz", full.names = TRUE)) # Just the forward read files
fnRs <- sort(list.files(trimmed_path, pattern="_R2_Nfilt.fastq.gz", full.names = TRUE)) # Just the reverse read files

# Get sample names from forward read filenames
sample.names <- 
   sapply(fnFs, function(x) gsub(".+trimmed/(.+)_R1.Nfilt.fastq.gz", "\\1", x), USE.NAMES = FALSE)

```

###quality profiles

Visualize the quality profile of the forward reads

```{r, "plot quality of reads", cache=TRUE}
# plot one forward and one reverse read in the rapport
plotQualityProfile(fnFs[grepl(test_sample, fnFs)])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[1], "foward", sep = " - " ))

plotQualityProfile(fnRs[grepl(gsub("_R1", "", test_sample), fnRs)])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[1], "reverse", sep = " - "))
```

The quality of the forwards reads is very high, throughout the full length of the read.  The quality of the reverse read deteriorates and the median quality drops below 30 after ~220 bps. 


### merged read length
To decide on appropriate trimming parameters, we check the distribution of the merged read-lengths.

we don't need the merged reads (DADA2 needs unmerged reads) so we merge them on them fly and just store the length distributions


```{r}
#merge first file with vsearch

  system(
  paste(
    "/Applications/vsearch/bin/vsearch --fastq_mergepairs ", fnFs[grepl(test_sample, fnFs)], 
    " --reverse ", fnRs[grepl(gsub("_R1", "", test_sample), fnRs)], 
    " --fastaout", here("Data", run_name, primer, "test_merge.fasta"), 
    "--fastq_allowmergestagger"
    ), intern = TRUE
  )
  
  #read file
file_merged <- readDNAStringSet(here("Data", run_name, primer, "test_merge.fasta"))


table(width(file_merged)) %>% 
  as.data.frame() %>% 
  mutate(across(one_of("Var1", "Freq"), function(x) as.numeric(as.character(x)))) %>% 
  arrange(desc(Freq)) %>% 
  mutate(prct = Freq / sum(Freq)*100) %>% 
  mutate(rank = 1:n()) %>% 
  mutate(Label = case_when( rank < 6 ~ paste(Var1, "(",round(prct,1), " %",")", sep = ""),
                   TRUE ~ NA_character_)) %>% 
    ggplot(aes(x = Var1, y = Freq))+
  geom_line(colour = "darkgrey", alpha = 0.5)+
  geom_text_repel(aes(label = Label))+
  geom_point(data = ~filter(.x, !is.na(Label)), colour = "red", size = 0.5)+
  theme_bw()+
  scale_x_continuous(limits = c(0,500))

  
```
Almost all fragment are exactly 313 bp long which is the expected length

The forward and reverse primers are 26bp long. Therefore the expected length of the reads is 274 bp respectively (which matches the length distribution of the primer trimmed reads above)

Thus we have a minimum overlap of `274+274-313` = 235 bp

The means that we can trim the ends of the reads without risking to to loose the overlap. 

```{r}
frag_len <- 313
fwrd_len <- 274
rev_len <- 274
fwrd_trim <- 220
rev_trim <- 180


ggplot(data = tibble(x = c(1,frag_len), y = c(1,5)), aes(x = x, y = y))+
  geom_blank()+
  geom_segment(x = 0, xend = frag_len, y = 2, yend = 2)+ #fragment
  geom_segment(x = 0, xend = fwrd_len, y = 3, yend = 3)+ #fwrd
  geom_segment(x = frag_len-rev_len, xend = frag_len, y = 3.5, yend = 3.5)+ #rev
  geom_segment(x = fwrd_trim, xend = fwrd_len, y = 3, yend = 3, 
               colour = "red", size = 2, alpha = 0.2)+ #fwrd trim
  geom_segment(x = frag_len-rev_trim, xend = frag_len-rev_len,
               y = 3.5, yend = 3.5, 
               colour = "red", size = 3, alpha = 0.2)+ #rev trim
  geom_segment(x = frag_len-rev_trim, xend = fwrd_trim, y = 2, 
               yend = 2, colour = "green", size= 3, alpha = 0.2)+ #overlap
  geom_text(x = ((frag_len-rev_trim) + fwrd_trim)/2, y = 2.25, 
            label = paste(as.character(fwrd_trim+rev_trim-frag_len), "bp"),
            colour = "blue", size= 5)+
  theme_void()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_x_continuous(breaks = seq(0,frag_len, 12))
```

The filtering parameters we’ll use are standard (however we can allow for more expected errors as DADA is [robust to low quality sequences](https://twitter.com/bejcal/status/771010634074820608) )

+ maxN=0 (DADA2 requires no Ns)
+ truncQ=2 
+ maxEE=c(2,2)

The `maxEE` parameter sets the maximum number of “expected errors” allowed in a read. 

We use the `fastqPairedFilter` function to jointly filter the forward and reverse reads.

###filter and trim
```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(filterpath, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filterpath, paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```


```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen = c(fwrd_trim, rev_trim),
                     maxN=0,
                     maxEE=c(2,2),
                     truncQ=2,
                     rm.phix=TRUE,
                     compress=TRUE, multithread=6) 
out_df <- 
out %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "Sample_name") %>% 
  mutate(Sample_name = gsub("_R1.+", "", Sample_name)) %>% 
  mutate(prct = signif(reads.out / reads.in * 100, 2)) %>% 
  mutate(Sample_name = gsub("(.+?)_S.+.fastq.gz", "\\1", Sample_name))

write_tsv(out_df, here("Data", run_name, primer, "ee_filter_stats.txt"))

out_df
```

```{r}
filt <- list.files(filterpath, full.names = T)

filtFs <- filt[grepl("*_F_filt.fastq.gz", filt)]
names(filtFs) <- gsub(".+filtered/(.+?)_F.+", "\\1", filtFs)

filtRs <- filt[grepl("*_R_filt.fastq.gz", filt)]
names(filtRs) <- gsub(".+filtered/(.+?)_R.+", "\\1", filtRs)
```


# Learn errors
```{r}
errF <- learnErrors(filtFs, multithread=6)
errR <- learnErrors(filtRs, multithread=6)
```


```{r, cache=TRUE}
plotErrors(errF, nominalQ=TRUE)
ggsave(paste(plotpath, "errorPlot_F.pdf", sep = "/"))

plotErrors(errR, nominalQ=TRUE)
ggsave(paste(plotpath, "errorPlot_R.pdf", sep = "/"))
```

# Denoise
```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```

```{r}

denoise_stat <- 
sapply(dadaFs, function(x) sum(getUniques((x)))) %>% 
  data.frame(denoised = .) %>% 
  tibble::rownames_to_column(var = "Sample_name") %>% 
  left_join(select(out_df, Sample_name, reads.out)) %>% 
  mutate(prct = round(denoised / reads.out*100, 2))
  
write_tsv(denoise_stat, here("Data", run_name, primer, "denoise_stat.txt"))

denoise_stat
```

# Merge
Merge paired reads
Spurious sequence variants are further reduced by merging overlapping reads. 

```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)

merge_stat <- 
lapply(mergers, function(x) {sum(x$abundance)}) %>% 
  as_tibble() %>% 
  pivot_longer(everything(),
    names_to = "Sample_name",
               values_to = "merged.out") %>% 
  left_join(select(out_df, Sample_name, reads.out)) %>% 
  mutate(prct = round(merged.out/reads.out * 100, 2))

merge_stat %>% 
write_tsv(here("Data", run_name, primer, "merge_stat"))

merge_stat
```

# Construct sequence table:

```{r, "Seqeunce table"}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```


# Chimeras

```{r, "Chimera removal", cache=TRUE}

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)

write_rds(seqtab.nochim, here("Data", run_name,
                              primer, "seqtab.nochim.rds"))

dim(seqtab.nochim)

```

```{r}
chimera_stat <- 
tibble(Sample_name = rownames(seqtab),
       before = rowSums(seqtab),
       after = rowSums(seqtab.nochim),
       prct = round(after/before*100,2))

chimera_stat %>% 
write_tsv(here("Data", run_name, primer, "Chimera_stat.txt"))

chimera_stat
```


percentage of Chimeric reads

```{r}
(1-(sum(seqtab.nochim)/sum(seqtab)))*100
```

percentage of Chimeric ASVs

```{r}
(1-(ncol(seqtab.nochim)/ncol(seqtab)))*100
```


# Length filtering

check ASV length

```{r}
data.frame(table(nchar(colnames(seqtab.nochim)))) %>% 
  ggplot( aes(x = Var1, y = Freq))+
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = -90))+
  labs(x = "length in bp", title = "histogramm of merged sequence lenghts")
```


# track reads

load statistic file from different steps
```{r}
N_filter_stats  <- read_tsv(here("Data", run_name, primer, "N_filter_stats.txt")) %>% 
  select(Sample_name, reads.in, reads.out) %>% 
  mutate(Sample_name = gsub("_R1.+", "", Sample_name))

Primer_trimming  <- read_tsv(here("Data", run_name, primer, "Primer_trimming_stats.txt")) %>% 
  select(Sample_name, Pairs_written)

ee_filter <- read_tsv(here("Data", run_name, primer, 
                           "ee_filter_stats.txt")) %>% 
  select(Sample_name, reads.out) %>% 
  dplyr::rename(ee_filt = reads.out)

denoise_stat <- read_tsv(here("Data", run_name, primer,
                              "denoise_stat.txt")) %>% 
  select(Sample_name, denoised) 
  
merge_stat <- read_tsv(here("Data", run_name, primer, "merge_stat")) %>% 
  select(Sample_name, merged.out) 

Chimera_stat  <- read_tsv(here("Data", run_name, primer, "Chimera_stat.txt")) %>% 
  select(Sample_name, after) 
```


```{r}
track <- 
  N_filter_stats %>% 
  left_join(Primer_trimming) %>% 
  left_join(ee_filter) %>% 
  left_join(denoise_stat) %>% 
  left_join(merge_stat) %>% 
  left_join(Chimera_stat) 
  

colnames(track) <- c("Sample_name", "input", "N_filtered", "primer_trimmed", "ee_filtered","denoised", "merged", "nonchim")

write_tsv(track, here("Data", run_name, primer, "full_stats.txt"))
track <- read_tsv(here("Data", run_name, primer, "full_stats.txt"))
```


```{r}
track %>% 
  pivot_longer(!one_of("Sample_name"), names_to = "step", values_to = "reads") %>% 
  mutate(step = factor(step, levels = c("input", "N_filtered",
                                        "primer_trimmed",
                                        "ee_filtered","denoised",
                                        "merged", "nonchim"))) %>%
  group_by(Sample_name) %>% 
  mutate(reads_prct = reads/max(reads)*100) %>% 
  pivot_longer(one_of(c("reads", "reads_prct"))) %>% 
  left_join(Sample_meta_min) %>% 
  ggplot(aes(x=step, y=value, group = Sample_name, colour = Sample_type))+
  geom_line(position = position_jitter(width = 0.1), alpha = 0.3, colour = "grey")+
  geom_point(position = position_jitter(width = 0.1),
             alpha = 0.8, shape = 21)+
  geom_violin(fill = NA, aes(group = step))+
  stat_summary(geom="line", fun.y="median", aes(group = 1), colour = "red")+
  facet_wrap(~name, scales = "free")+
  scale_y_continuous(limits = c(0,NA))+
  theme_bw()+
  theme(axis.text.x = element_text(angle = -30, hjust = 0),
        legend.position = "bottom")+
  scale_colour_brewer(palette = "Set1")+
  labs(y = "reads")

ggsave(here(plotpath, "sequence_filtering_abs.pdf"))
```

```{r}
track %>% 
  left_join(Sample_meta_min) %>%
  summarize(sum = sum(input),
            median = median(input),
            min = min(input),max = max(input),
            q25 = quantile(input, probs = 0.25),
            q75 = quantile(input, probs = 0.75))
```


###export raw ASV and fasta file

export ASVs to Fasta file 

```{r}
seqs <- DNAStringSet(colnames(seqtab.nochim))

names(seqs) <- paste("seq", 
                             formatC(1:length(seqs), flag = "0",
                                     width = nchar(length(seqs))),
                             sep = "_")

writeXStringSet(seqs, here("Data",run_name, primer, "ASVs.fasta"), format = "fasta")

#seqs <- readDNAStringSet(here("Data", run_name, primer, "ASVs.fasta"))

ASV <- 
seqtab.nochim %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "Sample") %>% 
  `colnames<-`(c("Sample", names(seqs))) 
  
write_tsv(ASV, here("Data", run_name, primer, "ASVs.txt"))
#ASV <- read_tsv(here("Data", run_name, primer, "ASVs.txt"))
```

###Swarm3

We use swarm for single linkage clustering of the ASVs. The method is described here:

> Mahé F, Rognes T, Quince C, Vargas C de, Dunthorn M. 2015 Swarm v2: highly-scalable and high-resolution amplicon clustering. PeerJ 3, e1420. (doi:10.7717/peerj.1420)

The github is [here](https://github.com/torognes/swarm)


Swarm needs a fasta file with size annotation as input. It just separates the size by a "_"

```{r}
seqs_size <- 
  colSums(ASV[,-1]) %>% 
  data.frame(size = .) %>% 
  tibble::rownames_to_column(var = "seq") %>% 
  mutate(new_name = gsub("_", "", seq)) %>% 
  mutate(new_name = paste(new_name, size, sep = "_"))

#make sure sequences are in correct order
seqs <- seqs[seqs_size$seq]

#rename
names(seqs) <- seqs_size$new_name

#export
writeXStringSet(seqs, here("Data", run_name, primer, "ASVs_swarm_in.fasta"), format = "fasta")

```


run swarm.

we use the usearch output as we already have code to convert usearch output into merged OTU tabel

```{r}
tmp <- 
system(paste(
  "/Applications/swarm-3.1.0/bin/swarm -d 1 -f -t 8 -w",
  here("Data", run_name, primer, "ASVs_swarm_out.fasta"), 
  "-u", here("Data",run_name, primer, "ASVs_swarm_cluster.txt"),
  here("Data", run_name, primer, "ASVs_swarm_in.fasta")
  , sep = " ")
  , intern = TRUE)

```


The ITS_ASVs_swarm_cluster.txt file tells us which sequences have been clustered

column 09 gives the original sequence name
column 10 gives the centroid sequence to which it has been clustered
column 01 tells us if the sequence is itself a centroid sequence ("S") or clustered to a centroid ("H")
column 04 gives the % identity with the centroid (here between 100% and 99%)

```{r}

Cluster_ID <- read_delim(here("Data", run_name, primer, "ASVs_swarm_cluster.txt"), delim = "\t", col_names = FALSE)

#filter only sequences that have been clustered
Cluster_ID <- 
Cluster_ID %>% 
  dplyr::rename(seq = X9, Type = X1, match = X10, pident = X4) %>% 
  select(seq, Type, match, pident) %>% 
  filter(Type == "H" ) %>% 
  arrange(match)

#rename seqs to match sequence table
Cluster_ID <- 
Cluster_ID %>% 
  mutate(across(one_of("seq", "match"), function(x){
    gsub("(seq)(\\d+)_\\d+", "\\1_\\2", x)}))

#split by cluster centroid
Cluster_list <- split(Cluster_ID, Cluster_ID$match)


#sum abundance of clustered sequences
ASV_list <- 
  lapply(Cluster_list, function(x) {
  S_seq <- unique(x$match)
  C_seqs <- x$seq
  ASV[,S_seq] <- rowSums(ASV[,c(S_seq,C_seqs)])
})

#create new ASV table with only the centroid sequences
ASV_sw <- ASV
ASV_sw[,names(ASV_list)] <- bind_rows(ASV_list)
ASV_sw <- ASV_sw[, which(! colnames(ASV_sw) %in% Cluster_ID$seq)] 

#export ASV table
ASV_sw %>% 
  write.table(., here("Data", run_name, primer, "ASV_swarm.text"))

```

update fasta file with sequence centroids
```{r}
seqs <- readDNAStringSet(here("Data", run_name, primer, "ASVs_swarm_out.fasta"))

names(seqs) <- gsub("(seq)(\\d+)_\\d+", "\\1_\\2", names(seqs))

sum(!names(seqs) %in% colnames(ASV_sw[,-1]))

writeXStringSet(seqs, here("Data",run_name, primer, "ASV_swarm.fasta"))
```

read_files
```{r}
seqs <- readDNAStringSet(here("Data",run_name, primer,  "ASV_swarm.fasta"))

ASV_sw <- read.table(here("Data", run_name, primer, "ASV_swarm.text"))
```

#taxonomy

1.Leray M, Knowlton N, Machida RJ. In press. MIDORI2: A collection of quality controlled, preformatted, and regularly updated reference databases for taxonomic assignment of eukaryotic mitochondrial sequences. Environmental DNA n/a. (doi:10.1002/edn3.303)


##Sintax

```{r}
system(paste(
  
  "/Applications/vsearch/bin/vsearch --sintax", here("Data", run_name, primer, "ASV_swarm.fasta"), "--db", here("Data", "MIDORI_UNIQ_NUC_GB249_CO1_SINTAX.fasta"), "--sintax_cutoff 0.1 --tabbedout", here("Data",run_name, primer, "COI_midori_tax.txt"), sep = " "

))
```

```{r}
Sintax_taxa <- read_tsv(here("Data",run_name, primer, "COI_midori_tax.txt"), 
                        col_names = FALSE) %>% 
  select(1:2)

colnames(Sintax_taxa) <- c("seq", "tax")

Sintax_prep <- 
  Sintax_taxa %>% 
  mutate(tax = strsplit(tax, ",")) %>% 
  unnest(tax) %>% 
  group_by(seq) %>% 
  dplyr::slice(-1) %>% 
  mutate(taxonomy_name = gsub("\\w:(.+)_.+", "\\1", tax),
         taxonomy_level = gsub("(\\w):.+", "\\1", tax),
         taxonomy_conf = gsub(".+\\((.+)\\)", "\\1", tax)) %>% 
  mutate(taxonomy_level = case_when(taxonomy_level == "p" ~ "phylum",
                   taxonomy_level == "c" ~ "class",
                   taxonomy_level == "o" ~ "order",
                   taxonomy_level == "f" ~ "family",
                   taxonomy_level == "g" ~ "genus",
                   taxonomy_level == "s" ~ "species"))

#get table with all assignments (even the unlikely ones)
Sintax_taxa_table_full <- 
 Sintax_prep %>% 
  select(-tax, -taxonomy_conf) %>% 
  pivot_wider(names_from = taxonomy_level, values_from = taxonomy_name)

#get table with probabilities for each assignment
Sintax_taxa_table_prob <- 
  Sintax_prep %>% 
  select(-tax, -taxonomy_name) %>% 
  pivot_wider(names_from = taxonomy_level, values_from = taxonomy_conf)
```

export

```{r}
write_tsv(Sintax_taxa_table_full, here("Data", "COI_Sintax_tax.txt"))
write_tsv(Sintax_taxa_table_prob, here("Data", "COI_Sintax_prob.txt"))

Sintax_taxa_table_full <- read_tsv(here("Data", "COI_Sintax_tax.txt"))
Sintax_taxa_table_prob <- read_tsv(here("Data", "COI_Sintax_prob.txt"))
```


```{r}
COI_taxa_80 <- Sintax_taxa_table_full
COI_taxa_80[Sintax_taxa_table_prob < 0.8] <- NA_character_ 

COI_taxa_80 <- 
  COI_taxa_80 %>% 
  mutate(across(everything(), function(x){gsub("NA", NA, x, fixed = TRUE)}))

write_tsv(COI_taxa_80, here("Data", run_name, primer, "COI_Sintax_80.txt"))
COI_taxa_80 <- read_tsv(here("Data", run_name, primer, "COI_Sintax_80.txt"))
```

```{r}
COI_taxa_80 %>% 
  group_by(phylum) %>% 
  summarise(n = n())

COI_taxa_80 %>% 
  filter(phylum == "Chordata")
```

## merge species

cluster sequences assigned to same species or genus (if species is NA)

collapse reads from same species
```{r}
#show same genus / species assigned to more than one OTU
  COI_taxa_80 %>% 
  group_by(genus, species) %>% 
  filter(n() > 1) %>% 
  arrange(genus, species)
```

we use phyloseq to merge ASVs with teh same taxonomic assignment at species level, or genius level if a species level assignment is missing

```{r}

ASV_ps <- ASV_sw[,-1]
rownames(ASV_ps) <- ASV_sw$Sample

COI_taxa_80_ps <- 
  COI_taxa_80 %>% 
   mutate(species = case_when(is.na(species) & !is.na(genus) ~ paste(genus, "sp."),
                             TRUE ~ species)) %>% 
  select(-seq) %>% 
  as.matrix()

rownames(COI_taxa_80_ps) <- COI_taxa_80$seq

ps_obj <- phyloseq(otu_table(ASV_ps, taxa_are_rows = FALSE),
                   tax_table(COI_taxa_80_ps))

ps_glom <- tax_glom(ps_obj, taxrank="species", NArm=FALSE)

COI_taxa_80_glom <- 
  as.data.frame(tax_table(ps_glom)) %>% 
  tibble::rownames_to_column("seq")

ASW_glom <- 
  as.data.frame(otu_table(ps_glom)) %>% 
  tibble::rownames_to_column("Sample")

```


export
```{r}
write_tsv(COI_taxa_80_glom, here("Data", "COI_taxa_80_glom.txt"))

write_tsv(ASW_glom, here("Data", "COI_ASW_glom.txt"))
```

